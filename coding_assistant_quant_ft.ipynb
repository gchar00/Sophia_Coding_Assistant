{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Quantization and Fine Tuning"
      ],
      "metadata": {
        "id": "WF_vP5A868Pu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFuSyn2o_0kQ"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install -q -U accelerate peft bitsandbytes transformers trl einops\n",
        "!pip install -q auto-gptq\n",
        "!pip install -q optimum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#optional\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9ZR2RPnaAoDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from datasets import load_from_disk\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "Ec7xRQniA3Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Set a fixed seed value\n",
        "set_seed(42)\n"
      ],
      "metadata": {
        "id": "mYw4cC8KBOOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load quantized model"
      ],
      "metadata": {
        "id": "vpTtEyosA8wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_name_or_path = \"TheBloke/phi-2-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=True,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "tokenizer.pad_token=tokenizer.eos_token\n",
        "tokenizer.padding_side=\"right\"\n"
      ],
      "metadata": {
        "id": "2wDkfRZFA3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to produce Alpaca1Tiny2 or Tiny1Alpaca2, load the initial model first"
      ],
      "metadata": {
        "id": "BS09qBX3AH0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/path_to_alpaca_model.pth'), strict=False)\n",
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/path_to_tiny_model.pth'), strict=False)"
      ],
      "metadata": {
        "id": "VaFsq0PgAQeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Datasets"
      ],
      "metadata": {
        "id": "j7LqgNQrBCiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alpaca"
      ],
      "metadata": {
        "id": "gy84A_tx2HtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"HuggingFaceH4/CodeAlpaca_20K\", split=\"train\")\n",
        "dataset = dataset.shuffle(seed=0)\n",
        "dataset = dataset.select(range(7450))\n",
        "\n",
        "dataset = dataset.rename_column('prompt', 'text')"
      ],
      "metadata": {
        "id": "VHzYfVsxBAAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiny codes"
      ],
      "metadata": {
        "id": "Dpyt-ojI2JXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Languages in tiny_codes\n",
        "languages = [\n",
        "           \"C++\",\n",
        "           \"Java\",\n",
        "          \"Ruby\",\n",
        "          \"Rust\",\n",
        "          \"Bash\",\n",
        "         ]\n"
      ],
      "metadata": {
        "id": "ue8ELBs_2LCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select 1490 samples from each programming language"
      ],
      "metadata": {
        "id": "WjxrxjP7_dq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "\n",
        "access_token = #your access token from hugging face\n",
        "\n",
        "dataset = load_dataset(\"nampdn-ai/tiny-codes\", split=\"train\", token=access_token)\n",
        "dataset = dataset.shuffle(seed=0)\n",
        "print(dataset)\n",
        "language_count = defaultdict(int)\n",
        "\n",
        "dataset31 = dataset.filter(lambda x: x['programming_language'] == languages[0])\n",
        "dataset32 = dataset.filter(lambda x: x['programming_language'] == languages[1])\n",
        "dataset33 = dataset.filter(lambda x: x['programming_language'] == languages[2])\n",
        "dataset34 = dataset.filter(lambda x: x['programming_language'] == languages[3])\n",
        "dataset35 = dataset.filter(lambda x: x['programming_language'] == languages[4])\n"
      ],
      "metadata": {
        "id": "MoUHxyx93Gjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code generation"
      ],
      "metadata": {
        "id": "t52Z_xRb_jG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset21 = dataset31.shuffle(seed=125)\n",
        "dataset21 = dataset31.select(range(1490))\n",
        "\n",
        "dataset22 = dataset32.shuffle(seed=125)\n",
        "dataset22 = dataset32.select(range(1490))\n",
        "\n",
        "dataset23 = dataset33.shuffle(seed=125)\n",
        "dataset23 = dataset33.select(range(1490))\n",
        "\n",
        "dataset24 = dataset34.shuffle(seed=125)\n",
        "dataset24 = dataset34.select(range(1490))\n",
        "\n",
        "dataset25 = dataset35.shuffle(seed=125)\n",
        "dataset25 = dataset35.select(range(1490))\n",
        "\n",
        "train_dataset1 = concatenate_datasets([dataset21, dataset22, dataset23, dataset24, dataset25])\n",
        "train_dataset1 = train_dataset1.shuffle(seed=125)\n",
        "\n",
        "train_dataset1 = train_dataset1.select_columns(['prompt','response'])\n",
        "dataset2 = train_dataset1.rename_column('prompt', 'text')\n",
        "\n",
        "print(dataset2)"
      ],
      "metadata": {
        "id": "k9SD9iXx_LzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code summarization"
      ],
      "metadata": {
        "id": "Qc57peha_oHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset31 = dataset31.shuffle(seed=125)\n",
        "dataset31 = dataset31.select(range(1490, 1490*2))\n",
        "\n",
        "dataset32 = dataset32.shuffle(seed=125)\n",
        "dataset32 = dataset32.select(range(1490, 1490*2))\n",
        "\n",
        "dataset33 = dataset33.shuffle(seed=125)\n",
        "dataset33 = dataset33.select(range(1490, 1490*2))\n",
        "\n",
        "dataset34 = dataset34.shuffle(seed=125)\n",
        "dataset34 = dataset34.select(range(1490, 1490*2))\n",
        "\n",
        "dataset35 = dataset35.shuffle(seed=125)\n",
        "dataset35 = dataset35.select(range(1490, 1490*2))\n",
        "\n",
        "train_dataset2 = concatenate_datasets([dataset31, dataset32, dataset33, dataset34, dataset35])\n",
        "train_dataset2 = train_dataset2.shuffle(seed=125)\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.select_columns(['prompt','response'])\n",
        "dataset3 = train_dataset.rename_column('response', 'text')\n",
        "dataset3 = dataset3.rename_column('prompt', 'response')\n",
        "\n",
        "print(dataset3)"
      ],
      "metadata": {
        "id": "PJH6z4Jq_Rbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenation of the two subdatasets(optional)"
      ],
      "metadata": {
        "id": "iYsM7sgw_pvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#if you want to fine-tune on one phase\n",
        "#dataset = concatenate_datasets([dataset2, dataset3])"
      ],
      "metadata": {
        "id": "bndM2OH6_aDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine-tuning process"
      ],
      "metadata": {
        "id": "rkwnXvh2BEzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1km0hVco_woQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You select the dataset and the model you want to finetune"
      ],
      "metadata": {
        "id": "LrmQvHLS_wxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir = \"./results\",\n",
        "    num_train_epochs = 8,\n",
        "    fp16 = False,\n",
        "    bf16 = False,\n",
        "    per_device_train_batch_size = 4,\n",
        "    per_device_eval_batch_size = 4,\n",
        "    gradient_accumulation_steps = 1,\n",
        "    gradient_checkpointing = False,\n",
        "    max_grad_norm = 0.3,\n",
        "    learning_rate = 2e-4,\n",
        "    weight_decay = 0.001,\n",
        "    optim = \"paged_adamw_32bit\",\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    max_steps = -1,\n",
        "    warmup_ratio = 0.03,\n",
        "    group_by_length = True,\n",
        "    save_steps = 500,\n",
        "    logging_steps = 200,\n",
        ")\n",
        "\n",
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha= 16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules= [\"Wqkv\", \"out_proj\"] #[\"Wqkv\", \"fc1\", \"fc2\" ] # [\"Wqkv\", \"out_proj\", \"fc1\", \"fc2\" ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # the model\n",
        "    train_dataset=dataset,  # the dataset\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length = 150, #100, 200\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")"
      ],
      "metadata": {
        "id": "KMhfbtRRBFpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "aUjKVh7lBRlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/path_to_model.pth')"
      ],
      "metadata": {
        "id": "xiGw2uJM-Wkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "loss_values = []\n",
        "for entry in trainer.state.log_history:\n",
        "  if 'loss' in entry.keys():\n",
        "    loss_values.append(entry['loss'])\n",
        "\n",
        "epochs = range(len(loss_values))\n",
        "\n",
        "plt.plot(epochs, loss_values)\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss - Model Fine Tuning')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "C1Xh8q419zFD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}